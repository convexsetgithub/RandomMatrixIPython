{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Elementary Matrix Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers the following topics:\n",
    "\n",
    "- [1.1 Matrix Norms](?kernel_name=python3#1.1-Matrix-Norms)\n",
    "- [1.2 Matrix Decomposition](?kernel_name=python3#1.2-Matrix-Decompositions)\n",
    " - [QR Decomposition](?kernel_name=python3#QR-Decomposition)\n",
    " - [Singular Value Decomposition (SVD)](?kernel_name=python3#Singular-Value-Decompostion)\n",
    " - [The Truncated SVD](?kernel_name=python3#The-Rank-k-Truncated-SVD)\n",
    " - [Eigenvalue Decomposition](?kernel_name=python3#Eigenvalue-Decompostion)\n",
    "- [1.3 Matrix Inverse and Pseudo-inverse](?kernel_name=python3#1.3-Matrix-Inverse-and-Pseudo-inverse)\n",
    " - [Matrix Inverse](?kernel_name=python3#Matrix-Inverse)\n",
    " - [Matrix Pseudo-inverse](?kernel_name=python3#Matrix-Pseudo-inverse)\n",
    " - [Least Squares Regression (LSR)](?kernel_name=python3#Least-Squares-Regression)\n",
    " \n",
    "You can skip this chapter if you are already with elementary matrix operations and NumPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate a $7\\times 4$ matrix ${\\bf A}$, whose entries are i.i.d. Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "\n",
      " [[-1.0856306   0.99734545  0.2829785  -1.50629471]\n",
      " [-0.57860025  1.65143654 -2.42667924 -0.42891263]\n",
      " [ 1.26593626 -0.8667404  -0.67888615 -0.09470897]\n",
      " [ 1.49138963 -0.638902   -0.44398196 -0.43435128]\n",
      " [ 2.20593008  2.18678609  1.0040539   0.3861864 ]\n",
      " [ 0.73736858  1.49073203 -0.93583387  1.17582904]\n",
      " [-1.25388067 -0.6377515   0.9071052  -1.4286807 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "matrixA = np.random.standard_normal([7, 4])\n",
    "print(\"A = \\n\\n\", matrixA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Matrix Norms\n",
    "\n",
    "The matrix Frobenius norm is defined by\n",
    "$$ \\| {\\bf A} \\|_F = \\sqrt{\\sum_{i j} a_{i j}^2} .$$\n",
    "\n",
    "The matrix spectral norm is defined by\n",
    "$$ \\| {\\bf A} \\|_2 \\; = \\; \\max_{{\\bf x} \\neq {\\bf 0}} \\frac{\\|{\\bf A} {\\bf x}\\|_2}{ \\| {\\bf x}\\|_2} .$$\n",
    "\n",
    "Let's see the Frobenius norm and spectral norm of ${\\bf A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Frobenius norm of A is: 6.33811103512\n",
      "The spectral norm of A is: 4.17282826794\n"
     ]
    }
   ],
   "source": [
    "frobeniusNormA = np.linalg.norm(matrixA, 'fro')\n",
    "print(\"The Frobenius norm of A is:\", frobeniusNormA)\n",
    "\n",
    "spectralNormA = np.linalg.norm(matrixA, 2)\n",
    "print(\"The spectral norm of A is:\", spectralNormA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Matrix Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "Let ${\\bf A}$ be an $m\\times n$ matrix with $m \\geq n$.\n",
    "The QR decomposition of ${\\bf A}$ is\n",
    "$$\n",
    "{\\bf A} \\; = \\; \\underbrace{{\\bf Q_A}}_{m\\times n} \\: \\underbrace{{\\bf R_A}}_{n\\times n}.\n",
    "$$\n",
    "The matrix ${\\bf Q_A}$ has orthonormal columns, that is,\n",
    "${{\\bf Q}_{\\bf A}^T} {\\bf Q_A} = {\\bf I}_n$.\n",
    "The matrix ${\\bf R_A}$ is upper triangular, that is, for all $i < j$, the $(i,j)$-th entry of ${\\bf R_A}$ is zero.\n",
    "It costs $O (m n^2)$ time to compute the QR decompostion.\n",
    "\n",
    "Let's compute the QR decomposition by NumPy ([numpy.linalg.qr](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.qr.html#numpy.linalg.qr))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = \n",
      "\n",
      " [[-0.30926928 -0.35898426  0.1875401  -0.53139597]\n",
      " [-0.16482889 -0.5183199  -0.68687346 -0.25868406]\n",
      " [ 0.3606339   0.33210471 -0.31425974 -0.29212158]\n",
      " [ 0.42485998  0.27965247 -0.22101851 -0.48596311]\n",
      " [ 0.62841486 -0.50081084  0.47145063 -0.18458948]\n",
      " [ 0.21005805 -0.38924632 -0.21486539  0.3611136 ]\n",
      " [-0.35719956  0.10794234  0.27747498 -0.40586871]] \n",
      "\n",
      "\n",
      "R = \n",
      "\n",
      " [[ 3.51030859  0.75048109 -0.01062386  1.31785587]\n",
      " [ 0.         -3.42479135  0.76593325 -0.19517926]\n",
      " [ 0.          0.          2.95750534 -0.32911936]\n",
      " [ 0.          0.          0.          2.08331573]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrixQ, matrixR = np.linalg.qr(matrixA, mode='reduced')\n",
    "print(\"Q = \\n\\n\", matrixQ, \"\\n\\n\")\n",
    "print(\"R = \\n\\n\", matrixR, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-off error:\n",
      "\n",
      "\t ||A - Q R|| =  1.15910686703e-15\n"
     ]
    }
   ],
   "source": [
    "# Verify the result by multiplying Q and R\n",
    "print(\"Round-off error:\\n\")\n",
    "print(\"\\t ||A - Q R|| = \", np.linalg.norm(matrixA - np.dot(matrixQ, matrixR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the columns of ${\\bf Q}$ are orthonormal.If so, we should have $\\|{\\bf I}_n - {\\bf Q}^T {\\bf Q}\\|_F = 0$. (However, due to the round-off error, it cannot be exactly zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-off error:\n",
      "\n",
      "\t ||I - Q' Q|| =  5.64934213789e-16\n"
     ]
    }
   ],
   "source": [
    "errorTerm = np.eye(matrixQ.shape[1]) - np.dot(matrixQ.transpose(), matrixQ)\n",
    "print(\"Round-off error:\\n\")\n",
    "print(\"\\t ||I - Q' Q|| = \", np.linalg.norm(errorTerm, 'fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decompostion\n",
    "\n",
    "Let ${\\bf A}$ be an $m\\times n$ matrix and $r = \\min \\{m, n\\}$.\n",
    "The singular value decomposition (SVD) of ${\\bf A}$ is\n",
    "$$\n",
    "\\underbrace{{\\bf A} }_{m\\times n}\n",
    "\\; = \\; \\underbrace{{\\bf U}_{\\bf A}}_{m\\times r} \n",
    "    \\: \\underbrace{{\\bf \\Sigma_A}}_{r \\times r} \n",
    "    \\: \\underbrace{{\\bf V}_{\\bf A}^T}_{r\\times n}\n",
    "\\; = \\; \\sum_{i = 1}^{\\rho} \\sigma_{{\\bf A},i} {\\bf u}_{{\\bf A},i} {\\bf v}_{{\\bf A} , i}^T.\n",
    "$$\n",
    "Here $\\sigma_{{\\bf A},1} \\geq \\cdots \\geq \\sigma_{{\\bf A},r} > 0$ are the singular values,\n",
    "${\\bf u}_{{\\bf A},1} , \\cdots , {\\bf u}_{{\\bf A},r} \\in \\mathbb{R}^m$ are the left singular vectors,\n",
    "and ${\\bf v}_{{\\bf A},1} , \\cdots , {\\bf v}_{{\\bf A},r} \\in \\mathbb{R}^n$ are the right singular vectors.\n",
    "\n",
    "Let's compute the SVD of ${\\bf A} = {\\bf U_A} {\\bf \\Sigma_A} {\\bf V}_{\\bf A}^T$ using NumPy ([numpy.linalg.svd](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html#numpy.linalg.svd)). The SVD costs $O (m n r)$ time, where $r = \\min \\{m, n\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      "\n",
      " [[-0.1863171   0.40039923 -0.45006716 -0.38010012]\n",
      " [ 0.2494457   0.79259629  0.27603966 -0.26058921]\n",
      " [ 0.10899075 -0.24297464  0.40506454 -0.43526038]\n",
      " [ 0.13150461 -0.26082502  0.27088106 -0.62050071]\n",
      " [ 0.63539313 -0.20851669 -0.63220244 -0.23488751]\n",
      " [ 0.49373833  0.19477821  0.07198984  0.29207641]\n",
      " [-0.47580287  0.05465757 -0.28093659 -0.26956198]] \n",
      "\n",
      "\n",
      "sigma = \n",
      "\n",
      " [ 4.17282827  3.39075521  2.77573984  1.88605493] \n",
      "\n",
      "\n",
      "V = \n",
      "\n",
      " [[-0.1863171   0.40039923 -0.45006716 -0.38010012]\n",
      " [ 0.2494457   0.79259629  0.27603966 -0.26058921]\n",
      " [ 0.10899075 -0.24297464  0.40506454 -0.43526038]\n",
      " [ 0.13150461 -0.26082502  0.27088106 -0.62050071]\n",
      " [ 0.63539313 -0.20851669 -0.63220244 -0.23488751]\n",
      " [ 0.49373833  0.19477821  0.07198984  0.29207641]\n",
      " [-0.47580287  0.05465757 -0.28093659 -0.26956198]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "matrixU, vectorSigma, matrixV = np.linalg.svd(matrixA, full_matrices=False)\n",
    "print(\"U = \\n\\n\", matrixU, \"\\n\\n\")\n",
    "print(\"sigma = \\n\\n\", vectorSigma, \"\\n\\n\")\n",
    "print(\"V = \\n\\n\", matrixU, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-off error:\n",
      "\n",
      "\t ||A - U diag(sigma) V'|| =  5.99700303564e-15\n"
     ]
    }
   ],
   "source": [
    "# Verify the result by multiplying U, Sigma, V'\n",
    "matrixUSV = np.dot(matrixU * vectorSigma, matrixV)\n",
    "print(\"Round-off error:\\n\")\n",
    "print(\"\\t ||A - U diag(sigma) V'|| = \", np.linalg.norm(matrixA - matrixUSV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of ${\\bf U}_{\\bf A}$ are the vectors ${\\bf u}_{{\\bf A},1} , \\cdots , {\\bf u}_{{\\bf A},\\rho} \\in \\mathbb{R}^m$. The columns of ${\\bf V}_{\\bf A}$ are the vectors ${\\bf v}_{{\\bf A},1} , \\cdots , {\\bf v}_{{\\bf A},\\rho} \\in \\mathbb{R}^m$.\n",
    "\n",
    "The matrices ${\\bf U}_{\\bf A}$ and ${\\bf V}_{\\bf A}$ have orthonormal columns. The readers are encouraged to verify that ${\\bf U}_{\\bf A}^T {\\bf U}_{\\bf A}= {\\bf I}_\\rho$\n",
    "and ${\\bf V}_{\\bf A}^T {\\bf V}_{\\bf A}= {\\bf I}_\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rank k Truncated SVD\n",
    "\n",
    "In applications such as the principal component analysis (PCA), latent semantic indexing (LSI), word2vec, spectral clustering,\n",
    "we are only interested in the top $k$ ($\\ll m, n$) singular values and singular vectors.\n",
    "The rank $k$ truncated SVD ($k$SVD) is denoted by\n",
    "$$\n",
    "{\\bf A}_k\n",
    "\\; = \\;\n",
    "\\sum_{i = 1}^{k} \\sigma_{{\\bf A},i} {\\bf u}_{{\\bf A},i} {\\bf v}_{{\\bf A} , i}^T\n",
    "\\; = \\; \\underbrace{{\\bf U}_{{\\bf A},k}}_{m\\times k} \\underbrace{{\\bf \\Sigma}_{{\\bf A},k}}_{k\\times k} \\underbrace{{\\bf V}_{{\\bf A},k}^T}_{k\\times n} .\n",
    "$$\n",
    "Here ${\\bf U}_{{\\bf A}, k}$ consists of the first $k$ singular vectors of ${\\bf U}_{\\bf A}$, and ${\\bf \\Sigma}_{{\\bf A},k}$ and ${\\bf V}_{{\\bf A},k}$ are analogously defined.\n",
    "Among all the $m\\times n$ rank $k$ matrices,\n",
    "${\\bf A}_k$ is the closest approximation to ${\\bf A}$ in that\n",
    "$$\n",
    "{\\bf A}_k \\; = \\;\n",
    "\\mathrm{argmin}_{{\\bf X}} \\|{\\bf A} - {\\bf X} \\|^2 ,\n",
    "\\qquad\n",
    "\\mathrm{s.t.} \\; \\mathrm{rank} ({\\bf X}) \\leq k.\n",
    "$$\n",
    "Here $\\|\\cdot\\|$ denote all unitarily invariant norms, including the Frobenius norm and the spectral norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue Decompostion\n",
    "\n",
    "The eigenvalue decomposition of an $n\\times n$ symmetric matrix ${\\bf A}$ is defined by\n",
    "$$\n",
    "{\\bf A}\n",
    "\\; = \\; {\\bf U}_{\\bf A} {\\bf \\Lambda}_{\\bf A} {\\bf U}_{\\bf A}^T\n",
    "\\; = \\; \\sum_{i=1}^n \\lambda_{{\\bf A},i} {\\bf u}_{{\\bf A} , i} {\\bf u}_{{\\bf A} , i}^T .\n",
    "$$\n",
    "Here $\\lambda_{{\\bf A},1} \\geq \\cdots \\geq \\lambda_{{\\bf A},n}$ are the eigenvalues of ${\\bf A}$,\n",
    "and ${\\bf u}_{{\\bf A},1} , \\cdots , {\\bf u}_{{\\bf A},n} \\in \\mathbb{R}^n$ are the corresponding eigenvectors.\n",
    "${\\bf A}$ is called symmetric positive semidefinite (SPSD) if and only if all the eigenvalues are greater than or equal to zero.\n",
    "If ${\\bf A}$ is SPSD,\n",
    "the SVD and eigenvalue decomposition of ${\\bf A}$ are identical.\n",
    "\n",
    "\n",
    "Let's compute the eigenvalue decomposition of symmetric matrix using NumPy ([numpy.linalg.eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html#numpy.linalg.eigh)). To begin with, we need to generate a symmetric matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      "\n",
      " [[-2.17126121  0.41874519  1.54891476 -0.01490509]\n",
      " [ 0.41874519  3.30287307 -3.29341965 -1.06781463]\n",
      " [ 1.54891476 -3.29341965 -1.3577723  -0.53869093]\n",
      " [-0.01490509 -1.06781463 -0.53869093 -0.86870255]]\n"
     ]
    }
   ],
   "source": [
    "# generate a 4-by-4 random matrix\n",
    "np.random.seed(123)\n",
    "matrixAsymmetric = np.random.standard_normal([4, 4])\n",
    "# make it symmetric\n",
    "matrixSymmetric = matrixAsymmetric + matrixAsymmetric.transpose()\n",
    "print(\"A = \\n\\n\", matrixSymmetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing the eigenvalue decomposition, we will obtain a vector of eigenvalues and a matrix of eigenvectors. But unlike numpy.linalg.svd, the resulting eigenvalues may not be sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = \n",
      "\n",
      " [-4.4164689  -1.51083014 -0.27273924  5.10517529] \n",
      "\n",
      "\n",
      "U = \n",
      "\n",
      " [[ 0.55857377 -0.70567163 -0.43359026  0.04497083]\n",
      " [-0.36442388 -0.24157492 -0.16794268 -0.88353384]\n",
      " [-0.71322675 -0.24155065 -0.47888806  0.45125061]\n",
      " [-0.21563436 -0.62074342  0.74462136  0.11712584]]\n"
     ]
    }
   ],
   "source": [
    "vectorLambda, matrixU = np.linalg.eigh(matrixSymmetric)\n",
    "print(\"lambda = \\n\\n\", vectorLambda, \"\\n\\n\")\n",
    "print(\"U = \\n\\n\", matrixU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result by computing $\\| {\\bf A} - {\\bf U} {\\bf \\Lambda} {\\bf U}^T\\|_F$.\n",
    "The result should be very close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-off error:\n",
      "\n",
      "\t ||A - U * diag(lambda) * U'|| =  3.15411656618e-15\n"
     ]
    }
   ],
   "source": [
    "# Verify the result by multiplying U, Lambda, U'\n",
    "matrixULU = np.dot(matrixU * vectorLambda, matrixU.transpose())\n",
    "print(\"Round-off error:\\n\")\n",
    "print(\"\\t ||A - U * diag(lambda) * U'|| = \", np.linalg.norm(matrixSymmetric - matrixULU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Matrix Inverse and Pseudo-inverse\n",
    "\n",
    "### Matrix Inverse\n",
    "For an $n\\times n$ square matrix ${\\bf A}$,\n",
    "the matrix inverse exists if ${\\bf A}$ is non-singular ($\\mathrm{rank} ({\\bf A}) = n$).\n",
    "Let ${\\bf A}^{-1}$ be the inverse of ${\\bf A}$.\n",
    "Then ${\\bf A} {\\bf A}^{-1} = {\\bf A}^{-1} {\\bf A} = {\\bf I}_n$.\n",
    "\n",
    "Let's compute the matrix inversion by ([numpy.linalg.inv](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html#numpy.linalg.inv)).\n",
    "We first generate a square matrix (not necessarily symmetric) and then computes its inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a 4-by-4 random matrix\n",
    "np.random.seed(123)\n",
    "matrixAsymmetric = np.random.standard_normal([4, 4])\n",
    "matrixInverse = np.linalg.inv(matrixAsymmetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find out what is ${\\bf A}^{-1} {\\bf A}$ and ${\\bf A} {\\bf A}^{-1}$.\n",
    "Hopefully, both of them should be the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv(A) * A = \n",
      "\n",
      " [[  1.00000000e+00   2.22044605e-16   0.00000000e+00   2.22044605e-16]\n",
      " [ -4.44089210e-16   1.00000000e+00   1.11022302e-16   2.22044605e-16]\n",
      " [ -2.22044605e-16   1.11022302e-16   1.00000000e+00   1.11022302e-16]\n",
      " [ -1.38777878e-16   9.71445147e-17  -2.77555756e-17   1.00000000e+00]] \n",
      "\n",
      "\n",
      "A * inv(A) = \n",
      "\n",
      " [[  1.00000000e+00  -8.32667268e-17   4.44089210e-16  -3.05311332e-16]\n",
      " [  5.55111512e-17   1.00000000e+00   2.77555756e-16  -3.60822483e-16]\n",
      " [  2.77555756e-17  -1.21430643e-17   1.00000000e+00  -8.32667268e-17]\n",
      " [  0.00000000e+00  -6.93889390e-18   2.77555756e-16   1.00000000e+00]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"inv(A) * A = \\n\\n\", np.dot(matrixInverse, matrixAsymmetric), \"\\n\\n\")\n",
    "print(\"A * inv(A) = \\n\\n\", np.dot(matrixAsymmetric, matrixInverse), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Pseudo-inverse\n",
    "\n",
    "\n",
    "Only square and nonsingular (aka.full rank) matrices have inverse.\n",
    "For the general rectangular matrices or rank deficient matrices,\n",
    "matrix pseudo-inverse is used as a generalization of matrix inverse.\n",
    "\n",
    "Let ${\\bf A}$ be any $m\\times n$ matrix and $\\rho = \\mathrm{rank}({\\bf A}) \\leq m, n$.\n",
    "Let \n",
    "$$\n",
    "{\\bf A} \\; =\\; \n",
    "\\underbrace{{\\bf U_A}}_{m\\times \\rho} \n",
    "\\:\\underbrace{{\\bf \\Sigma_A}}_{\\rho\\times \\rho} \n",
    "\\:\\underbrace{{\\bf V}_{\\bf A}^T}_{\\rho\\times n}\n",
    "$$ \n",
    "be the condensed SVD, which means that only the non-zero singular values are retained.\n",
    "The Moore-Penrose inverse is the most widely used pseudo-inverse, which is defined by\n",
    "$$\n",
    "{\\bf A}^{\\dagger} \\; = \\;\n",
    "{\\bf V}_{{\\bf A}} {\\bf \\Sigma}_{\\bf A}^{-1} {\\bf U}_{{\\bf A}}^T \n",
    "\\; \\in \\; \\mathbb{R}^{n\\times m}.\n",
    "$$\n",
    "Suppose that $m \\geq n$. Then it holds that\n",
    "$$\n",
    " {\\bf A}^{\\dagger} {\\bf A} = {\\bf I}_n .\n",
    "$$\n",
    "However, in general\n",
    "$$\n",
    " {\\bf A} {\\bf A}^{\\dagger} \\neq {\\bf I}_m .\n",
    "$$\n",
    "The matrix ${\\bf A} {\\bf A}^{\\dagger} $ is called the orthogonal projector because ${\\bf A} {\\bf A}^{\\dagger} {\\bf B}$ projects ${\\bf B}$ to the column space of ${\\bf A}$.\n",
    "\n",
    "Let's compute the matrix pseudo-inverse by the NumPy function [numpy.linalg.pinv](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html#numpy.linalg.pinv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a 4-by-3 matrix\n",
    "np.random.seed(123)\n",
    "matrixA = np.random.standard_normal([4, 3])\n",
    "matrixPinv = np.linalg.pinv(matrixA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find out what is ${\\bf A}^{\\dagger} {\\bf A}$ and ${\\bf A} {\\bf A}^{\\dagger}$.\n",
    "Only the former should be identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinv(A) * A = \n",
      "\n",
      " [[  1.00000000e+00   4.99600361e-16  -3.40005801e-16]\n",
      " [  2.77555756e-16   1.00000000e+00  -2.15105711e-16]\n",
      " [  0.00000000e+00   2.77555756e-16   1.00000000e+00]] \n",
      "\n",
      "\n",
      "A * pinv(A) = \n",
      "\n",
      " [[ 0.87576168 -0.15866457  0.22077876 -0.18677647]\n",
      " [-0.15866457  0.7973697   0.28195624 -0.23853195]\n",
      " [ 0.22077876  0.28195624  0.60766322  0.33191272]\n",
      " [-0.18677647 -0.23853195  0.33191272  0.7192054 ]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"pinv(A) * A = \\n\\n\", np.dot(matrixPinv, matrixA), \"\\n\\n\")\n",
    "print(\"A * pinv(A) = \\n\\n\", np.dot(matrixA, matrixPinv), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Regression\n",
    "\n",
    "Suppose we are given $n$ data points of $d$ dimension, each datum is associated with a vector of ``response''.\n",
    "Let each row of ${\\bf X} \\in \\mathbb{R}^{n\\times d}$ be a datum and each row of ${\\bf Y} \\in \\mathbb{R}^{n\\times b}$ be the associated response.Assume that $n \\geq d, b$. We hope to find a matrix ${\\bf W} \\in \\mathbb{R}^{d\\times b}$ such that ${\\bf X} {\\bf W} \\approx {\\bf Y}$. The most straightforward approach is to solve the least squares regression (LSR) problem:\n",
    "$$\n",
    "{\\bf W}^\\star \\; = \\;\n",
    "\\| {\\bf X} {\\bf W} - {\\bf Y} \\|_F^2 .\n",
    "$$\n",
    "This problem has closed form solution:\n",
    "$$\n",
    "{\\bf W}^\\star \\; = \\;{\\bf X}^{\\dagger}  {\\bf Y} .\n",
    "$$\n",
    "Let's check it using NumPy. We generate a $7\\times 4$ data matrix ${\\bf X}$ and a $4\\times 2$ model matrix ${\\bf W}$. \n",
    "The response matrix is generated by\n",
    "$$\n",
    "{\\bf Y}\n",
    "\\; = \\; {\\bf X} {\\bf W} + \\mathrm{Noise} .\n",
    "$$\n",
    "We will check if the solution ${\\bf W}^\\star={\\bf X}^{\\dagger}  {\\bf Y}$ is close to the ground truth ${\\bf W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (true) = \n",
      "\n",
      " [[-0.14006872 -0.8617549 ]\n",
      " [-0.25561937 -2.79858911]\n",
      " [-1.7715331  -0.69987723]\n",
      " [ 0.92746243 -0.17363568]] \n",
      "\n",
      "\n",
      "W (solved by pseudo-inverse) = \n",
      "\n",
      " [[-0.14973502 -0.8727113 ]\n",
      " [-0.24295372 -2.78692194]\n",
      " [-1.75762877 -0.69008784]\n",
      " [ 0.94772657 -0.19226652]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "# Generate a 7-by-4 matrix X\n",
    "matrixX = np.random.standard_normal([7, 4])\n",
    "# Generate a 4-by-2 matrix W\n",
    "matrixWTrue = np.random.standard_normal([4, 2])\n",
    "# Compute Y by Y = X * W + Noise\n",
    "noiseRate = 0.04\n",
    "matrixY = np.dot(matrixX, matrixWTrue) + noiseRate * np.random.standard_normal([7, 2])\n",
    "print(\"W (true) = \\n\\n\", matrixWTrue, \"\\n\\n\")\n",
    "\n",
    "# Solve the Least Squares Regression problem: min_W ||X*W - Y||\n",
    "matrixXPinv = np.linalg.pinv(matrixX)\n",
    "matrixWSolved = np.dot(matrixXPinv, matrixY)\n",
    "print(\"W (solved by pseudo-inverse) = \\n\\n\", matrixWSolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the solution ${\\bf W}^\\star$ is pretty close to the ground truth ${\\bf W}$.\n",
    "In fact, as ``noiseRate'' goes to zero, the solution should be exact.\n",
    "\n",
    "In the above, we use the matrix pseudo-inverse to find the solution. However, this method is slow and instable. In practice people usually solve LSR by the conjugate gradient algorithm. It is implemented as the NumPy function [numpy.linalg.lstsq](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq).\n",
    "\n",
    "We show the result of [numpy.linalg.lstsq](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq) in the blow. We can see that the result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (solved by lstsq) = \n",
      "\n",
      " [[-0.14973502 -0.8727113 ]\n",
      " [-0.24295372 -2.78692194]\n",
      " [-1.75762877 -0.69008784]\n",
      " [ 0.94772657 -0.19226652]]\n"
     ]
    }
   ],
   "source": [
    "# Solve the Least Squares Regression problem: min_W ||X*W - Y||\n",
    "matrixWSolved = np.linalg.lstsq(matrixX, matrixY)[0]\n",
    "print(\"W (solved by lstsq) = \\n\\n\", matrixWSolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
