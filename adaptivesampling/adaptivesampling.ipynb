{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling\n",
    "\n",
    "**Shusen Wang**,\n",
    "wssatzju@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations\n",
    "\n",
    "We first prepare the data for our experiments. \n",
    "\n",
    "Let us load a matrix from the file '../data/letters.txt'. The matix ${\\bf X}\\in \\mathbb{R}^{15,000\\times 16}$ has $15,000$ samples and $16$ features. Let ${\\bf X}_1$ contain the first $2,000$ rows and ${\\bf X}_2$ contain the next $5,000$ rows. We then form the $2,000\\times 5,000$ kernel matrix ${\\bf A}$ using the rbf function.\n",
    "\n",
    "\n",
    "The rbf function is defined in 'rbf.py'. We describe it in the following.\n",
    "- Input:\n",
    "    * ${\\bf X}_1 \\in \\mathbb{R}^{n_1\\times d}$ and ${\\bf X}_2 \\in \\mathbb{R}^{n_2\\times d}$: each row of which is $d$ dimensional data sample;\n",
    "    * $\\sigma > 0$: the kernel width parameter.\n",
    "- Output:\n",
    "    * ${\\bf K} \\in \\mathbb{R}^{n_1\\times n_2}$: the kernel matrix. Let ${\\bf x}_1$ be the $i$-th row of ${\\bf X}_1$ and ${\\bf x}_2$ be the $j$-th row of ${\\bf X}_2$. Then the $(i,j)$-th entry of ${\\bf K}$ is\n",
    "    $$k_{i j} = \\exp \\bigg( - \\frac{ \\|{\\bf x}_1 - {\\bf x}_2 \\|_2^2 }{ 2 \\sigma^2 } \\bigg) .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix A has  2000  rows and  5000  columns.\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "import numpy as np\n",
    "from rbf import rbf\n",
    "\n",
    "# load matrix X from file\n",
    "filepath = '../data/letters.txt'\n",
    "matrixX = np.loadtxt(filepath)\n",
    "\n",
    "# form the RBF kernel matrix\n",
    "matrixX1 = matrixX[0:2000, :]\n",
    "matrixX2 = matrixX[2000:7000, :]\n",
    "sigma = 0.7\n",
    "matrixA = rbf(matrixX1, matrixX2, sigma)\n",
    "print('The matrix A has ', matrixA.shape[0], ' rows and ', matrixA.shape[1], ' columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Description\n",
    "\n",
    "In a word, adaptive sampling is to select columns according to the residual of the previous round. Adaptive sampling can be described in the following.\n",
    "\n",
    "1. Input: ${\\bf A}\\in \\mathbb{R}^{m\\times n}$, ${\\bf C}_1 \\in \\mathbb{R}^{m\\times c_1}$, $c_2$.\n",
    "2. Compute the residual ${\\bf B} = {\\bf A} - {\\bf C}_1 {\\bf C}_1^{\\dagger} {\\bf A}$;\n",
    "3. Compute the sampling probabilities: $p_j = \\frac{\\|{\\bf b}_{:j}\\|_2^2}{\\|{\\bf B}\\|_F^2}$ for $j=1$ to $n$;\n",
    "4. Sample $c_2$ columns of ${\\bf A}$ according to $p_1 , \\cdots , p_n$ to form ${\\bf C}_2 \\in \\mathbb{R}^{m\\times c_2}$;\n",
    "5. Output: ${\\bf C} = [ {\\bf C}_1 , {\\bf C}_2 ] \\in \\mathbb{R}^{m\\times (c_1+c_2)}$, which the stack of ${\\bf C}_1$ and ${\\bf C}_2$. \n",
    "\n",
    "The following Python code implements the adaptive sampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adaptiveSampling(matrixA, matrixC1, c2):\n",
    "    # compute the residual\n",
    "    matrixQ, matrixR = np.linalg.qr(matrixC1, mode='reduced')\n",
    "    matrixQQA = np.dot(matrixQ, np.dot(matrixQ.transpose(), matrixA))\n",
    "    matrixRes = matrixA - matrixQQA\n",
    "    # compute the sampling probabilites\n",
    "    matrixRes = np.square(matrixRes)\n",
    "    prob = sum(matrixRes)\n",
    "    prob = prob / sum(prob)\n",
    "    return np.random.choice(matrixA.shape[1], c2, replace=False, p=prob)\n",
    "\n",
    "c1 = 100;\n",
    "c2 = 200;\n",
    "# matrix C1 contains c1 uniformly sampled columns of matrix A\n",
    "indexC1 = np.random.choice(matrixA.shape[1], c1, replace=False)\n",
    "matrixC1 = matrixA[:, indexC1]\n",
    "indexC2 = adaptiveSampling(matrixA, matrixC1, c2)\n",
    "matrixC2 = matrixA[:, indexC2]\n",
    "matrixC = np.concatenate((matrixC1, matrixC2), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the error ratio $\\frac{ \\| {\\bf A} - {\\bf C} {\\bf C}^\\dagger {\\bf A} \\|_F }{ \\|{\\bf A}\\|_F }$. Notice that ${\\bf A} - {\\bf C} {\\bf C}^\\dagger {\\bf A}$ is the error incurred by projecting ${\\bf A}$ to the column space of ${\\bf C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0395156611257\n"
     ]
    }
   ],
   "source": [
    "matrixQ, matrixR = np.linalg.qr(matrixC, mode='reduced')\n",
    "matrixRes = matrixA - np.dot(matrixQ, np.dot(matrixQ.T, matrixA))\n",
    "approxError = np.linalg.norm(matrixRes, 'fro') / np.linalg.norm(matrixA, 'fro')\n",
    "print(approxError)\n",
    "del(matrixR, matrixQ, matrixRes, approxError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Theory\n",
    "\n",
    "The theoretical properties of adaptive sampling has been analyzed in the following two papers:\n",
    "> Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projective clustering via volume sampling. *Theory of Computing*, 2(2006): 225-247, 2006.\n",
    "\n",
    "> Shusen Wang and Zhihua Zhang. Improving CUR matrix decomposition and the Nystrom approximation via adaptive sampling. *Journal of Machine Learning Research*, 14: 2729-2769, 2013.\n",
    "\n",
    "Let $k \\ll m, n$ be any fixed integer.Deshpande et al. showed that\n",
    "$$\\mathbb{E} \\big\\| {\\bf A} - {\\bf C} {\\bf C}^\\dagger {\\bf A} \\big\\|_F^2\n",
    "\\; \\leq \\; \\min_{\\textrm{rank} ({\\bf M}) \\leq k} \\big\\| {\\bf A} - {\\bf M} \\big\\|_F^2\n",
    "+ \\frac{k}{c_2} \\big\\| {\\bf A} - {\\bf C}_1 {\\bf C}_1^\\dagger {\\bf A} \\big\\|_F^2 ,$$\n",
    "where the expectation is taken w.r.t. the randomness in ${\\bf C}_2$.\n",
    "\n",
    "\n",
    "Let ${\\bf R}$ be any fixed matrix of proper size and $\\rho = \\textrm{rank} ({\\bf R})$.\n",
    "Wang & Zhang showed that\n",
    "$$\\mathbb{E} \\big\\| {\\bf A} - {\\bf C} {\\bf C}^\\dagger {\\bf A} {\\bf R}^\\dagger {\\bf R} \\big\\|_F^2\n",
    "\\; \\leq \\;  \\big\\| {\\bf A} -  {\\bf A} {\\bf R}^\\dagger {\\bf R}\\big\\|_F^2\n",
    "+ \\frac{\\rho}{c_2} \\big\\| {\\bf A} - {\\bf C}_1 {\\bf C}_1^\\dagger {\\bf A} \\big\\|_F^2 ,$$\n",
    "where the expectation is also taken w.r.t. the randomness in ${\\bf C}_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drawbacks of Adaptive Sampling\n",
    "\n",
    "The adaptive sampling algorithm has very strong error bound and good empirical performance, but it is computationally inefficient. Specifically, it has two major disadvantages.\n",
    "- It requires forming the residual matrix ${\\bf A} - {\\bf C}_1 {\\bf C}_1^\\dagger {\\bf A}$, which costs $O (m n c_1)$ time.\n",
    "- Every entry of ${\\bf A}$ must be visited. As for kernel methods, it is prohibitive to form the whole kernel matrix when the number of data sample is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. The Incomplete Adaptive Sampling Algorithm\n",
    "\n",
    "To overcome the two disadvantages of adaptive sampling, an efficient heuristic has been proposed by the following paper. \n",
    "> Shusen Wang, Luo Luo, and Zhihua Zhang. SPSD matrix approximation vis column selection: theories, algorithms, and extensions. *Journal of Machine Learning Research*, 17(49):1âˆ’49, 2016. (JMLR 2016).\n",
    "\n",
    "The basic idea is to first uniformly sample $o(n)$ columns and then down-sample to $c_2$ columns by adaptive sampling. The algorithm can be described as follows.\n",
    "1. Input: ${\\bf A}\\in \\mathbb{R}^{m\\times n}$, ${\\bf C}_1 \\in \\mathbb{R}^{m\\times c_1}$, $c_2$.\n",
    "2. Set an oversampling parameter $s > c_2$ and uniformly sample $s$ columns of ${\\bf A}$ to form $\\tilde{\\bf A} \\in \\mathbb{R}^{m\\times s}$;\n",
    "3. Compute the residual ${\\bf B} = \\tilde{\\bf A} - {\\bf C}_1 {\\bf C}_1^{\\dagger} \\tilde{\\bf A}$;\n",
    "4. Compute the sampling probabilities: $p_j = \\frac{\\|{\\bf b}_{:j}\\|_2^2}{\\|{\\bf B}\\|_F^2}$ for $j=1$ to $s$;\n",
    "5. Sample $c_2$ columns of $\\tilde{\\bf A}$ according to $p_1 , \\cdots , p_s$ to form ${\\bf C}_2 \\in \\mathbb{R}^{m\\times c_2}$;\n",
    "6. Output: ${\\bf C} = [ {\\bf C}_1 , {\\bf C}_2 ] \\in \\mathbb{R}^{m\\times (c_1+c_2)}$, which the stack of ${\\bf C}_1$ and ${\\bf C}_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def incompleteAdaptiveSampling(matrixA, matrixC1, c2):\n",
    "    n = matrixA.shape[1]\n",
    "    # oversampling (uniformly)\n",
    "    s = min(n, 5 * c2) # the oversampling parameter can be tuned\n",
    "    indexAtilde = np.random.choice(n, s, replace=False)\n",
    "    matrixAtilde = matrixA[:, indexAtilde]\n",
    "    # compute the residual\n",
    "    matrixQ, matrixR = np.linalg.qr(matrixC1, mode='reduced')\n",
    "    matrixQQAtilde = np.dot(matrixQ, np.dot(matrixQ.transpose(), matrixAtilde))\n",
    "    matrixRes = matrixAtilde - matrixQQAtilde\n",
    "    # compute the sampling probabilites\n",
    "    matrixRes = np.square(matrixRes)\n",
    "    prob = sum(matrixRes)\n",
    "    prob = prob / sum(prob)\n",
    "    indexC2 = np.random.choice(s, c2, replace=False, p=prob)\n",
    "    return indexAtilde[indexC2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Experiments\n",
    "\n",
    "In the following we compare adaptive sampling with the incomplete adaptive sampling in terms of the error ratio $\\frac{ \\| {\\bf A} - {\\bf C} {\\bf C}^\\dagger {\\bf A} \\|_F }{ \\|{\\bf A}\\|_F }$. We also employ uniform sampling for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error of the uniform sampling is  0.04232081367\n",
      "The error of the incomplete adaptive sampling is  0.0392436200861\n",
      "The error of the adaptive sampling is  0.0393426495711\n"
     ]
    }
   ],
   "source": [
    "c1 = 100;\n",
    "c2 = 200;\n",
    "c = c1 + c2;\n",
    "normA = np.linalg.norm(matrixA, 'fro')\n",
    "\n",
    "# matrix C1 contains c1 uniformly sampled columns of matrix A\n",
    "indexC1 = np.random.choice(matrixA.shape[1], c1, replace=False)\n",
    "matrixC1 = matrixA[:, indexC1]\n",
    "\n",
    "# repeat the experiments 10 times\n",
    "repeat = 10\n",
    "errorUniform = list()\n",
    "errorIncomplete = list()\n",
    "errorAdaptive = list()\n",
    "for i in range(repeat):\n",
    "    # uniform sampling\n",
    "    indexC = np.random.choice(matrixA.shape[1], c, replace=False)\n",
    "    matrixC = matrixA[:, indexC]\n",
    "    matrixQ, matrixR = np.linalg.qr(matrixC, mode='reduced')\n",
    "    matrixRes = matrixA - np.dot(matrixQ, np.dot(matrixQ.T, matrixA))\n",
    "    errorUniform.append(np.linalg.norm(matrixRes, 'fro') / normA)\n",
    "    del(indexC, matrixC, matrixQ, matrixR, matrixRes)\n",
    "    # incomplete adaptive sampling\n",
    "    indexC2 = incompleteAdaptiveSampling(matrixA, matrixC1, c2)\n",
    "    matrixC2 = matrixA[:, indexC2]\n",
    "    matrixC = np.concatenate((matrixC1, matrixC2), 1)\n",
    "    matrixQ, matrixR = np.linalg.qr(matrixC, mode='reduced')\n",
    "    matrixRes = matrixA - np.dot(matrixQ, np.dot(matrixQ.T, matrixA))\n",
    "    errorIncomplete.append(np.linalg.norm(matrixRes, 'fro') / normA)\n",
    "    del(indexC2, matrixC2, matrixC, matrixQ, matrixR, matrixRes)\n",
    "    # adaptive sampling\n",
    "    indexC2 = adaptiveSampling(matrixA, matrixC1, c2)\n",
    "    matrixC2 = matrixA[:, indexC2]\n",
    "    matrixC = np.concatenate((matrixC1, matrixC2), 1)\n",
    "    matrixQ, matrixR = np.linalg.qr(matrixC, mode='reduced')\n",
    "    matrixRes = matrixA - np.dot(matrixQ, np.dot(matrixQ.T, matrixA))\n",
    "    errorAdaptive.append(np.linalg.norm(matrixRes, 'fro') / normA)\n",
    "    del(indexC2, matrixC2, matrixC, matrixQ, matrixR, matrixRes)\n",
    "\n",
    "print('The error of the uniform sampling is ', np.mean(errorUniform))\n",
    "print('The error of the incomplete adaptive sampling is ', np.mean(errorIncomplete))\n",
    "print('The error of the adaptive sampling is ', np.mean(errorAdaptive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment shows that the incomplete adaptive sampling is as good as the standard adaptive sampling and that they are both better than uniform sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
